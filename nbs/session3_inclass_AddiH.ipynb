{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ```spaCy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different NLP frameworks that you're likely to encounter. The most popular and widely-used of these are:\n",
    "\n",
    "- ```NLTK``` (Natural Language Toolkit, old-school)\n",
    "- ```UDPipe``` (Neural network based, fast and light, but not super accurate)\n",
    "- ```CoreNLP``` and ```stanza``` (Created by the team at Stanford; academically robust)\n",
    "- ```spaCy``` production-ready, well-documented, state-of-the-art\n",
    "\n",
    "We'll be working with ```spaCy``` in this module, primarily because it's easy and intuitive, and also scales well.\n",
    "\n",
    "First thing we need to do is install ```spaCy``` and the language model that we want to use.\n",
    "\n",
    "From the command line, you should first make sure to run the setup script to install requirements:\n",
    "\n",
    "```shell \n",
    "bash setup.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing ```spaCy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is import ```spaCy``` __and__ the language model that we want to use.\n",
    "\n",
    "Note that, if you want to use different langauges you want to use different language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spacy NLP class\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model now loaded, we can begin to do some very simple NLP tasks.\n",
    "\n",
    "Here, we create a spaCy object and assign it to the variable ```nlp```. This is the NLP pipeline that will do all our heavy lifting, using the trained model we've specified.\n",
    "\n",
    "Below, you can see what the pipeline does with a bit of sample text. Passing text to the nlp object gives us access to a bunch of properties, including tokens (words), parts of speech, named entities, and so on. Here's we two of them, tokens and entities. These objects, in turn, have certain methods attached to them. A full outline of available methods can be found in the spaCy docs.\n",
    "\n",
    "In this case, for all token objects, let's return the token itself (```token.text```); its part-of-speech tag (```token.pos_```); and the grammatical dependency relations between the tokens (```token.dep_```).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single sentence example\n",
    "doc = nlp(\"wow is it working? blerg is my cat, and she likes my sister\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenize__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow\n",
      "is\n",
      "it\n",
      "working\n",
      "?\n",
      "blerg\n",
      "is\n",
      "my\n",
      "cat\n",
      ",\n",
      "and\n",
      "she\n",
      "likes\n",
      "my\n",
      "sister\n"
     ]
    }
   ],
   "source": [
    "# tokenizing text\n",
    "for token in doc:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Trying some more attributes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 wow INTJ intj <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "1 is AUX aux <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "2 it PRON nsubj <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "3 working VERB ROOT <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "4 ? PUNCT punct <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "5 blerg PROPN nsubj <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "6 is AUX ROOT <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "7 my DET poss <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "8 cat NOUN attr <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "9 , PUNCT punct <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "10 and CCONJ cc <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "11 she PRON nsubj <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "12 likes VERB conj <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "13 my DET poss <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n",
      "14 sister NOUN dobj <spacy.tokens.morphanalysis.MorphAnalysis object at 0x7fe2c0e8f048>\n"
     ]
    }
   ],
   "source": [
    "# find parts-of-speech and grammatical relations\n",
    "for token in doc:\n",
    "    print(token.i, token.text, token.pos_, token.dep_, token.morph) \n",
    "    #pos = part of speech, dep = dependency, _ gives word codes intead of just numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NER__\n",
    "\n",
    "Extracting named entities from a ```spaCy``` doc requires an extra step, but nothing too challenging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blerg PERSON\n"
     ]
    }
   ],
   "source": [
    "# extracting NERs\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Questions:__ \n",
    "\n",
    "1. What range of linguistic features is available beyond what we're looking at here? \n",
    "2. Are the same range of features available for all languages? Compare e.g. English and Danish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count distribution of linguistic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create doc object__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a text file\n",
    "import os\n",
    "filename = os.path.join(\"..\", \"data\", \"example.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a doc object\n",
    "with open(filename, \"r\", encoding = \"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Western Asia', '7500', 'two to five', 'the United Kingdom', '2017', 'second', '2021', 'the United States', 'Felidae', 'Egypt', 'spring', 'About 60', 'around 3100', 'an estimated 220 million', '480 million', '26%', '10.9 million', 'at least one', 'around 42 million', '95.6 million'}\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "entities = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    entities.append(ent.text)\n",
    "    \n",
    "    \n",
    "print(set(entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of adjectives\n",
    "\n",
    "adj_count = 0\n",
    "for token in doc:\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        adj_count += 1\n",
    "\n",
    "adj_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relative frequency__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the relative frequency per 10,000 words\n",
    "rel_freq = (adj_count/len(doc)) * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1091.31"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rel_freq, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating neater outputs using ```pandas```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, all of our output from ```spaCy``` is in the form of lists. If we want to save these, it probably makes sense to have them saved in a more transferable format, such as CSV files or JSONs.\n",
    "\n",
    "One very easy way to do this with Python is by using the dataframe library ```pandas```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spaCy doc\n",
    "doc = nlp(\"wow is it working? blerg is my cat, and she likes my sister\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = []\n",
    "\n",
    "for token in doc:\n",
    "    anno.append((token.text, token.pos_)) # ()is a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Piece Of Shit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wow</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>working</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>?</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blerg</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>my</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cat</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>she</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>likes</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>my</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sister</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Token Piece Of Shit\n",
       "0       wow          INTJ\n",
       "1        is           AUX\n",
       "2        it          PRON\n",
       "3   working          VERB\n",
       "4         ?         PUNCT\n",
       "5     blerg         PROPN\n",
       "6        is           AUX\n",
       "7        my           DET\n",
       "8       cat          NOUN\n",
       "9         ,         PUNCT\n",
       "10      and         CCONJ\n",
       "11      she          PRON\n",
       "12    likes          VERB\n",
       "13       my           DET\n",
       "14   sister          NOUN"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spaCy doc to pandas dataframe\n",
    "data = pd.DataFrame(anno, columns = [\"Token\", \"Piece Of Shit\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "outpath = os.path.join(\"..\", \"out\", \"anno.csv\")\n",
    "data.to_csv(outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "\n",
    "Spend some time exploring and familiarizing yourself with ```spaCy``` and ```pandas```. We'll come back to them quite a lot through this semester, so it will help to have a solid handle on how they function.\n",
    "\n",
    "When you are ready, head over to [Assignment 1](https://classroom.github.com/a/PdNi7nPv) which takes some of the skills you've learned last week and today. The task will be to count how many times certain linguistic features occur accross different documents, and to save those results in a clear and easy-to-understand way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Feb 28 2021, 17:03:44) \n[GCC 10.2.1 20210110]"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
